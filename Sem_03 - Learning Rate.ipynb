{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxa de Aprendizado (Learning rate)\n",
    "\n",
    "A taxa de aprendizado é um hiperparâmetro que controla o quanto os pesos da rede são ajustados em resposta ao gradiente calculado em cada iteração.\n",
    "\n",
    "Considerações Importantes:\n",
    "\n",
    "•\tProgramação da Taxa de Aprendizado: É comum usar uma taxa de aprendizado que diminui ao longo do tempo. No início do treinamento, uma taxa mais alta pode ajudar a rede a explorar a paisagem de perda de forma mais agressiva, mas conforme o treinamento progride, uma taxa mais baixa pode ajudar a refinar a solução encontrada, evitando que o algoritmo oscile em torno do mínimo.\n",
    "\n",
    "•\tTentativa e Erro: A escolha da programação correta para a taxa de aprendizado (como ela diminui ao longo do tempo) geralmente requer experimentação. Técnicas como learning rate schedules (agendamentos de taxa de aprendizado) ou adaptive learning rates (taxas de aprendizado adaptativas) podem ser usadas para melhorar a convergência.\n",
    "\n",
    "- Sobre: \" **lr_scheduler = LearningRateScheduler(lr_schedule)**:\n",
    "\n",
    "    - é usada para criar um callback que ajusta a taxa de aprendizado durante o treinamento do modelo. O Learning Rate Scheduler é uma técnica que ajusta dinamicamente a taxa de aprendizado (learning rate) ao longo do treinamento de uma rede neural. A taxa de aprendizado é um hiperparâmetro importante que determina o tamanho dos passos que o otimizador dá ao ajustar os pesos da rede com base no gradiente calculado.\n",
    "\n",
    "    - Função LearningRateScheduler: é um tipo específico de callback no Keras que ajusta a taxa de aprendizado de acordo com uma função definida pelo usuário a cada época do treinamento.\n",
    "\n",
    "- lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_schedule: Esta é a função que você define para especificar como a taxa de aprendizado deve ser ajustada ao longo do treinamento. \n",
    "A função lr_schedule(epoch) retorna diferentes valores para a taxa de aprendizado com base na época atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "x_train = x_train.astype('float32') / 256.0\n",
    "x_test = x_test.astype('float32') / 256.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Transformar os rótulos em categorias (one-hot encoding)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Carlos Carneiro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "# Definir o modelo\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir uma função de programação da taxa de aprendizado. Escolha o valor de l_r\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 0.001\n",
    "    if epoch < 5:\n",
    "        return initial_lr\n",
    "    elif epoch < 10:\n",
    "        return initial_lr * 0.5\n",
    "    else:\n",
    "        return initial_lr * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar o modelo com otimizador Adam e uma taxa de aprendizado inicial. Escolha o valor l_r\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Definir o callback de programação da taxa de aprendizado\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8796 - loss: 0.4044 - val_accuracy: 0.9852 - val_loss: 0.0472 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m320/938\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.9849 - loss: 0.0521"
     ]
    }
   ],
   "source": [
    "# Executando 15 épocas. Tamanho dos mini lotes: 64 (64 amostras por vez antes de calcular o gradiente e ajustar os pesos).\n",
    "# escolher isso também para melhorar o resultado de perda e acurácia.\n",
    "\n",
    "# Treinar o modelo\n",
    "history = model.fit(x_train, y_train, epochs=15, batch_size=64,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[lr_scheduler])\n",
    "\n",
    "# Avaliar o modelo\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f'\\nTest accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Plotar os gráficos de perda e acurácia\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Gráfico de perda\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history['loss'], 'r-', label='Perda de Treino')\n",
    "plt.plot(epochs, history.history['val_loss'], 'b-', label='Perda de Validação')\n",
    "plt.title('Perda ao Longo das Épocas')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico de acurácia\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history['accuracy'], 'r-', label='Acurácia de Treino')\n",
    "plt.plot(epochs, history.history['val_accuracy'], 'b-', label='Acurácia de Validação')\n",
    "plt.title('Acurácia ao Longo das Épocas')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
