{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para entender como a retropropagação (backpropagation) aplica a diferenciação em modo reverso utilizando a regra da cadeia no contexto de redes neurais profundas, vamos implementar um código passo a passo que ilustra o processo de retropropagação em uma rede neural simples.\n",
    "\n",
    "- Cenário:\n",
    "Vamos considerar uma rede neural com:\n",
    "\n",
    "Uma camada de entrada com 2 neurônios.\n",
    "Uma camada oculta com 2 neurônios e função de ativação sigmoide.\n",
    "Uma camada de saída com 1 neurônio e função de ativação sigmoide.\n",
    "\n",
    "- Objetivo:\n",
    "Demonstrar como a retropropagação é usada para calcular os gradientes e ajustar os pesos da rede, aplicando a regra da cadeia “de fora para dentro”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Funções auxiliares\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 1: Inicialização dos Pesos\n",
    "np.random.seed(42)\n",
    "# Pesos para a camada oculta\n",
    "W1 = np.random.rand(2, 2)\n",
    "# Pesos para a camada de saída\n",
    "W2 = np.random.rand(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 2: Definindo o dataset de entrada (X) e saída esperada (y)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 3: Forward Pass (Passagem para frente)\n",
    "\n",
    "hidden_input = np.dot(X, W1)               # Entrada para a camada oculta\n",
    "\n",
    "hidden_output = sigmoid(hidden_input)      # Saída da camada oculta (após ativação)\n",
    "\n",
    "output_input = np.dot(hidden_output, W2)   # Entrada para a camada de saída\n",
    "\n",
    "predicted_output_before = sigmoid(output_input)   # Saída da camada de saída (após ativação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saída prevista antes da retropropagação:\n",
      " [[0.53892274]\n",
      " [0.55132394]\n",
      " [0.5510619 ]\n",
      " [0.56117033]]\n"
     ]
    }
   ],
   "source": [
    "# Exibindo a saída prevista antes da retropropagação\n",
    "print(\"\\nSaída prevista antes da retropropagação:\\n\", predicted_output_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 4: Calculando o erro\n",
    "error = y - predicted_output_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 5: Retropropagação (Backpropagation)\n",
    "d_output = error * sigmoid_derivative(predicted_output_before) # Derivada do erro em relação à saída da camada de saída\n",
    "error_hidden_layer = d_output.dot(W2.T)\n",
    "d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualização dos pesos\n",
    "W2 += hidden_output.T.dot(d_output)  # Atualizando pesos da camada de saída\n",
    "W1 += X.T.dot(d_hidden_layer)        # Atualizando pesos da camada oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos atualizados W1:\n",
      " [[0.3746971  0.95108266]\n",
      " [0.73176459 0.59950624]]\n",
      "Pesos atualizados W2:\n",
      " [[0.12596692]\n",
      " [0.12678416]]\n",
      "\n",
      "Saída prevista após a retropropagação:\n",
      " [[0.5315519 ]\n",
      " [0.54162756]\n",
      " [0.54142998]\n",
      " [0.54965008]]\n",
      "\n",
      "Valores esperados (y):\n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Passo 6: Forward Pass novamente para obter a saída prevista após a retropropagação\n",
    "hidden_input_after = np.dot(X, W1)               # Entrada para a camada oculta\n",
    "hidden_output_after = sigmoid(hidden_input_after)  # Saída da camada oculta (após ativação)\n",
    "output_input_after = np.dot(hidden_output_after, W2)  # Entrada para a camada de saída\n",
    "predicted_output_after = sigmoid(output_input_after)  # Saída prevista após a retropropagação\n",
    "\n",
    "# Exibindo os pesos atualizados\n",
    "print(\"\\nPesos atualizados W1:\\n\", W1)\n",
    "print(\"Pesos atualizados W2:\\n\", W2)\n",
    "\n",
    "# Exibindo a saída prevista após a retropropagação\n",
    "print(\"\\nSaída prevista após a retropropagação:\\n\", predicted_output_after)\n",
    "\n",
    "# Exibindo os valores esperados\n",
    "print(\"\\nValores esperados (y):\\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos analisar os resultados da retropropagação após uma iteração de treinamento.\n",
    "\n",
    "\"predicted_output = sigmoid(output_input)\" forneciam os valores de saída esperados.\n",
    "\n",
    "1. Pesos Atualizados (W1 e W2):\n",
    "\n",
    "- W1: São os pesos entre a camada de entrada e a camada oculta. \n",
    "     - Antes da retropropagação, esses pesos tinham valores iniciais aleatórios.\n",
    "     - Após uma iteração de retropropagação, esses pesos foram ligeiramente ajustados para minimizar o erro entre a saída prevista e a saída esperada.\n",
    "     - Os novos valores de W1 indicam como a rede está ajustando a importância de cada entrada na produção da saída da camada oculta.\n",
    "- W2: São os pesos entre a camada oculta e a camada de saída.\n",
    "\n",
    "Assim como W1, os pesos W2 foram atualizados para minimizar o erro de previsão. Os valores de W2 após a atualização mostram o quanto cada neurônio na camada oculta contribui para a saída final.\n",
    "\n",
    "\n",
    "2. Saída Prevista (predicted_output):\n",
    "\n",
    "- Essa é a saída da rede neural após a primeira iteração de treinamento, antes de comparar com os valores esperados (y).\n",
    "\n",
    "- Valores:\n",
    "[[0.5315519 ]\n",
    " [0.54162756]\n",
    " [0.54142998]\n",
    " [0.54965008]]\n",
    "\n",
    "    - Esses valores são os resultados da função sigmoide aplicada ao somatório ponderado das saídas da camada oculta. Eles representam a probabilidade prevista pela rede para cada uma das entradas (X), variando de 0 a 1. Como estamos usando uma função de ativação sigmoide, a saída sempre estará nesse intervalo.\n",
    "\n",
    "\n",
    "3. Interpretação Geral:\n",
    "\n",
    "- Convergência Inicial: Após uma única iteração, as saídas previstas estão começando a se ajustar para aproximar os valores corretos de y. No entanto, como houve apenas uma iteração, as previsões ainda não são muito próximas dos valores esperados.\n",
    "- Ajuste Incremental: O aprendizado profundo, especialmente em redes com várias camadas, é incremental. A cada iteração (ou época), a retropropagação ajusta os pesos um pouco mais, movendo as previsões na direção correta.\n",
    "- Próximas Iterações: Com mais iterações, os pesos continuarão sendo ajustados, o que deve levar as saídas previstas a se aproximarem dos valores esperados.\n",
    "- Este é um exemplo do processo de aprendizado na rede neural. A retropropagação ajusta os pesos para melhorar a precisão da rede ao prever a saída correta. Este processo é repetido muitas vezes até que o erro entre a saída prevista e a saída esperada seja minimizado o máximo possível."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
